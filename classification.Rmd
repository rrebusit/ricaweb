---
title: "Stroke Prediction"
author: "Rica Rebusit"
date: "5/11/2022"
output: 
  html_document:
    theme: darkly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(caret)
set.seed(1234)
```
## Introduction
Stroke is one of the world's leading causes in death. In order to protect the well-being of an individual, it is important to be able to accurately predict the likelihood of someone having a stroke based on their conditions to take action early and avoid long term damage. In this blog post, I will be documenting my process in using logistic regression in the tidy models framework to predict stroke using this raw data set from kaggle: [Click Me](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)

## Variables

This data set has 5110 rows of 12 variables

*id*: The patient's identity number

*gender*: The gender the patient identifies as. 'Male', 'Female', or 'Other'

*age*: Age of the patient in years

*hypertension*: If patient has hypertension or high blood pressure. 0 for no and 1 for yes

*heart_disease*: If patient has heart disease. 0 for no and 1 for yes

*ever_married*: If the patient has been married. Yes for yes and no for no

*work_type*: The type of employment the patient is in. 'Private', 'Self-employed', 'Government job', 'Never_worked', or people who do not have a job which is 'children'

*Residence_type*: The type of area where the patient lives. 'Urban' or 'Rural'

*average_glucose_level*: Average glucose level of the patient. Measures the level of sugar in a patient's blood and tests for diabetes. The normal average glucose varies by age.

*bmi*: Body Mass Index of patient 

*smoking_status*: If the patient smokes, never smokes, smoked before, or unknown

*stroke*: If the patient had a stroke. 0 for no and 1 for yes

## Cleaning Up

First we will need to clean the data set if needed so that the data is not messy

Let's load in the data set
```{r}
stroke <- read.csv("~/Downloads/healthcare-dataset-stroke-data.csv")
```

Now we will check the summary statistics
```{r}
summary(stroke)
```

The data needs a little cleaning so let's check out the type the variables are
```{r}
str(stroke)
```

The data has variables that are characters or or integers that need to be converted into factors that way we can fit the logistic regression
```{r, warning = FALSE}
stroke$stroke <- as.factor(stroke$stroke)
stroke$gender <- as.factor(stroke$gender)
stroke$hypertension <- as.factor(stroke$hypertension)
stroke$heart_disease <- as.factor(stroke$heart_disease)
stroke$ever_married <- as.factor(stroke$ever_married)
stroke$work_type <- as.factor(stroke$work_type)
stroke$Residence_type <- as.factor(stroke$Residence_type)
stroke$smoking_status <- as.factor(stroke$smoking_status)
stroke$bmi <- as.numeric(stroke$bmi)
```

Let's look at the data again now 
```{r}
summary(stroke)
```

I'll just remove the *id* column because it's not really needed
```{r}
stroke <- select(stroke, -c("id"))
```

## Quick Exploratory Data Analysis

Let's look at some quick initial graphs to visualize the data
```{r}
ggplot(stroke, aes(stroke, fill=stroke)) + geom_bar()
```

Seems like an imbalance in data. More people have not had a stroke than people who do

```{r}
ggplot(stroke, aes(age, avg_glucose_level, color = stroke)) + geom_point(alpha = 0.4) + facet_wrap(~stroke)
```

People who have had strokes are mostly elderly

# Classification - Logistic Regression

Now that we cleaned and did some exploration on our data, we can fit a model. Although more cleaning need to be done before we can have a better model such as imputing the NA values in bmi which would enable us to retain more of the data and do some resampling because the amount of people recorded having no stroke is much greater than people that do. However for this blog post, we will just keep it raw and see how the model performs.

```{r}
par.spec <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm")
lm.fit <- par.spec %>%
  fit(stroke ~ age + gender + hypertension + heart_disease + avg_glucose_level + bmi + smoking_status, data=stroke)
tidy(lm.fit)
```

Now that we have fitted our model, let's see how well it performs 

```{r}
pred <- predict(lm.fit, stroke)
confusionMatrix(stroke$stroke, pred$.pred_class, positive = "1")
```

# Conclusion
It looks like the model accurately predicted 95% of the time, however this is still not a good model. A lot of techniques should be done in order to have a better model. Like mentioned previously, we would need to do some resampling. We would also have to do imputations for the missing values. We should also use machine learning to train and test the data because it would be ideal to use a model to predict future data points as well as to account for overfitting

# About
Hi there, my name is Rica Rebusit and I am an Applied Mathematics major at California State University, Chico also in the Data Science program. I am interested in working in the healthcare or sports fields, but I am open to other fields.

Here are my links to my [Website](https://data485-s22.github.io/website-basic-rrebusit/), [Handshake](https://csuchico.joinhandshake.com/stu/users/16836139), [GitHub](https://github.com/rrebusit)

```{r}
sessionInfo()
```

